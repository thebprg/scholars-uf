{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_ufl_scholars():\n",
        "    \"\"\"Fetch all UF scholars data with pagination\"\"\"\n",
        "\n",
        "    cookies = {\n",
        "        'cookieConsent': '',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Accept-Language': 'en-US,en;q=0.6',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Origin': 'https://scholars.ufl.edu',\n",
        "        'Referer': 'https://scholars.ufl.edu/search?back&by=text&type=user&v=',\n",
        "        'Sec-Fetch-Dest': 'empty',\n",
        "        'Sec-Fetch-Mode': 'cors',\n",
        "        'Sec-Fetch-Site': 'same-origin',\n",
        "        'Sec-GPC': '1',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',\n",
        "        'accept': 'application/json',\n",
        "        'content-type': 'application/json',\n",
        "        'sec-ch-ua': '\"Brave\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': '\"macOS\"',\n",
        "    }\n",
        "\n",
        "    all_resources = []\n",
        "    per_page = 100\n",
        "    start_from = 0\n",
        "    total_records = None\n",
        "\n",
        "    print(f\"Starting data extraction at {datetime.now()}\")\n",
        "    print(f\"Records per page: {per_page}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        json_data = {\n",
        "            'params': {\n",
        "                'by': 'text',\n",
        "                'category': 'user',\n",
        "                'text': '',\n",
        "            },\n",
        "            'pagination': {\n",
        "                'startFrom': start_from,\n",
        "                'perPage': per_page,\n",
        "            },\n",
        "            'sort': 'lastNameAsc',\n",
        "            'filters': [\n",
        "                {\n",
        "                    'name': 'tags',\n",
        "                    'matchDocsWithMissingValues': True,\n",
        "                    'useValuesToFilter': False,\n",
        "                },\n",
        "                {\n",
        "                    'name': 'department',\n",
        "                    'matchDocsWithMissingValues': True,\n",
        "                    'useValuesToFilter': False,\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"Fetching records {start_from} to {start_from + per_page}...\")\n",
        "            response = requests.post(\n",
        "                'https://scholars.ufl.edu/api/users',\n",
        "                cookies=cookies,\n",
        "                headers=headers,\n",
        "                json=json_data,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            # Extract resources from this page\n",
        "            if 'resource' in data:\n",
        "                resources = data['resource']\n",
        "\n",
        "                # If no resources returned, we've reached the end\n",
        "                if not resources:\n",
        "                    print(\"✓ No more records to fetch\")\n",
        "                    break\n",
        "\n",
        "                all_resources.extend(resources)\n",
        "                print(f\"✓ Fetched {len(resources)} records. Total so far: {len(all_resources)}\")\n",
        "            else:\n",
        "                print(\"⚠ No 'resource' field in response\")\n",
        "                break\n",
        "\n",
        "            # Get total from first response\n",
        "            if total_records is None and 'pagination' in data:\n",
        "                total_records = data['pagination'].get('total', 0)\n",
        "                print(f\"ℹ Total records available: {total_records}\")\n",
        "                print(f\"ℹ Estimated pages: {(total_records + per_page - 1) // per_page}\")\n",
        "\n",
        "            # Move to next page\n",
        "            start_from += per_page\n",
        "\n",
        "            # Check if we've fetched everything\n",
        "            if total_records and start_from >= total_records:\n",
        "                print(\"✓ All records fetched\")\n",
        "                break\n",
        "\n",
        "            # Don't wait after the last request\n",
        "            if total_records and start_from >= total_records:\n",
        "                break\n",
        "\n",
        "            print(f\"Waiting 10 seconds before next request...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"✗ Error fetching data at startFrom={start_from}: {e}\")\n",
        "            print(\"Saving partial data...\")\n",
        "            break\n",
        "\n",
        "    # Save to JSON file (just the resources array)\n",
        "    filename = f\"ufl_scholars_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_resources, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"✓ Data extraction complete!\")\n",
        "    print(f\"Total records fetched: {len(all_resources)}\")\n",
        "    print(f\"Data saved to: {filename}\")\n",
        "\n",
        "    return all_resources\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fetch_ufl_scholars()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K2nx7ZLwWnbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# Function based on get_email.py logic\n",
        "def get_email(discovery_url_id):\n",
        "    cookies = {\n",
        "        'cookieConsent': '',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Accept-Language': 'en-US,en;q=0.6',\n",
        "        'Connection': 'keep-alive',\n",
        "        'If-None-Match': 'W/\"3c9-E4Kn9QarD8ifcyXpzLDtE1pAFmk\"',\n",
        "        # Update referer dynamically\n",
        "        'Referer': f'https://scholars.ufl.edu/{discovery_url_id}/publications',\n",
        "        'Sec-Fetch-Dest': 'empty',\n",
        "        'Sec-Fetch-Mode': 'cors',\n",
        "        'Sec-Fetch-Site': 'same-origin',\n",
        "        'Sec-GPC': '1',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',\n",
        "        'accept': 'application/json',\n",
        "        'content-type': 'application/json',\n",
        "        'sec-ch-ua': '\"Brave\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': '\"macOS\"',\n",
        "    }\n",
        "\n",
        "    url = f'https://scholars.ufl.edu/api/users/{discovery_url_id}'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, cookies=cookies, headers=headers, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            # Extract email from the specific JSON path provided by user\n",
        "            email_data = data.get('emailAddress', {})\n",
        "            if isinstance(email_data, dict):\n",
        "                return email_data.get('address', '')\n",
        "            return ''\n",
        "        else:\n",
        "            print(f\"Failed to fetch {discovery_url_id}: Status {response.status_code}\")\n",
        "            return ''\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {discovery_url_id}: {e}\")\n",
        "        return ''\n",
        "\n",
        "def process_emails(input_csv, output_csv):\n",
        "    print(f\"Reading from {input_csv}...\")\n",
        "\n",
        "    with open(input_csv, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames\n",
        "\n",
        "        # Add 'email' to fieldnames if not present\n",
        "        if 'email' not in fieldnames:\n",
        "            fieldnames.append('email')\n",
        "\n",
        "        rows = list(reader)\n",
        "        total = len(rows)\n",
        "        print(f\"Found {total} entries. Starting extraction...\")\n",
        "\n",
        "        with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for i, row in enumerate(rows):\n",
        "                discovery_id = row.get('discoveryUrlId')\n",
        "\n",
        "                if discovery_id:\n",
        "                    email = get_email(discovery_id)\n",
        "                    row['email'] = email\n",
        "                    if email:\n",
        "                        print(f\"[{i+1}/{total}] Found email for {discovery_id}: {email}\")\n",
        "                    else:\n",
        "                        print(f\"[{i+1}/{total}] No email found for {discovery_id}\")\n",
        "                else:\n",
        "                    print(f\"[{i+1}/{total}] Skipping row with missing discoveryUrlId\")\n",
        "                    row['email'] = ''\n",
        "\n",
        "                writer.writerow(row)\n",
        "\n",
        "                # Add delay to avoid rate limiting\n",
        "                time.sleep(1)\n",
        "\n",
        "    print(f\"Done! Results saved to {output_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"ufl_scholars_data.csv\"\n",
        "    output_file = \"ufl_scholars_data_with_emails.csv\"\n",
        "\n",
        "    process_emails(input_file, output_file)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-ZjExBBTIsdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "def get_publications(discovery_url_id):\n",
        "    url = 'https://scholars.ufl.edu/api/publications/linkedTo'\n",
        "\n",
        "    cookies = {\n",
        "        'cookieConsent': '',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Accept-Language': 'en-US,en;q=0.6',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Origin': 'https://scholars.ufl.edu',\n",
        "        'Referer': f'https://scholars.ufl.edu/{discovery_url_id}/publications',\n",
        "        'Sec-Fetch-Dest': 'empty',\n",
        "        'Sec-Fetch-Mode': 'cors',\n",
        "        'Sec-Fetch-Site': 'same-origin',\n",
        "        'Sec-GPC': '1',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',\n",
        "        'accept': 'application/json',\n",
        "        'content-type': 'application/json',\n",
        "        'sec-ch-ua': '\"Brave\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': '\"macOS\"',\n",
        "    }\n",
        "\n",
        "    # Using the exact payload structure provided\n",
        "    json_data = {\n",
        "        'objectId': discovery_url_id,\n",
        "        'category': 'user',\n",
        "        'pagination': {\n",
        "            'perPage': 100, # Increased limit to capture more data if possible, though user example had 25\n",
        "            'startFrom': 0,\n",
        "        },\n",
        "        'sort': 'dateDesc',\n",
        "        'favouritesFirst': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, cookies=cookies, headers=headers, json=json_data, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            # Return the raw text as requested \"whole json text\"\n",
        "            return response.text\n",
        "        else:\n",
        "            print(f\"Failed to fetch publications for {discovery_url_id}: Status {response.status_code}\")\n",
        "            return '{}'\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching publications for {discovery_url_id}: {e}\")\n",
        "        return '{}'\n",
        "\n",
        "def get_grants(discovery_url_id):\n",
        "    url = 'https://scholars.ufl.edu/api/grants/linkedTo'\n",
        "\n",
        "    cookies = {\n",
        "        'cookieConsent': 'WzAsZmFsc2Vd',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Accept-Language': 'en-US,en;q=0.6',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Origin': 'https://scholars.ufl.edu',\n",
        "        'Referer': f'https://scholars.ufl.edu/{discovery_url_id}/grants',\n",
        "        'Sec-Fetch-Dest': 'empty',\n",
        "        'Sec-Fetch-Mode': 'cors',\n",
        "        'Sec-Fetch-Site': 'same-origin',\n",
        "        'Sec-GPC': '1',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',\n",
        "        'accept': 'application/json',\n",
        "        'content-type': 'application/json',\n",
        "        'sec-ch-ua': '\"Brave\";v=\"143\", \"Chromium\";v=\"143\", \"Not A(Brand\";v=\"24\"',\n",
        "        'sec-ch-ua-mobile': '?0',\n",
        "        'sec-ch-ua-platform': '\"macOS\"',\n",
        "    }\n",
        "\n",
        "    json_data = {\n",
        "        'objectId': discovery_url_id,\n",
        "        'category': 'user',\n",
        "        'pagination': {\n",
        "            'perPage': 100, # Increased limit\n",
        "            'startFrom': 0,\n",
        "        },\n",
        "        'sort': 'dateDesc',\n",
        "        'favouritesFirst': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, cookies=cookies, headers=headers, json=json_data, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            print(f\"Failed to fetch grants for {discovery_url_id}: Status {response.status_code}\")\n",
        "            return '{}'\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching grants for {discovery_url_id}: {e}\")\n",
        "        return '{}'\n",
        "\n",
        "def process_research_data(input_csv, output_csv):\n",
        "    print(f\"Reading from {input_csv}...\")\n",
        "\n",
        "    if not os.path.exists(input_csv):\n",
        "        print(f\"Error: Input file '{input_csv}' not found.\")\n",
        "        return\n",
        "\n",
        "    with open(input_csv, 'r', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames\n",
        "\n",
        "        # Add new columns\n",
        "        if 'publications_json' not in fieldnames:\n",
        "            fieldnames.append('publications_json')\n",
        "        if 'grants_json' not in fieldnames:\n",
        "            fieldnames.append('grants_json')\n",
        "\n",
        "        rows = list(reader)\n",
        "        total = len(rows)\n",
        "        print(f\"Found {total} entries. Starting extraction...\")\n",
        "\n",
        "        with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for i, row in enumerate(rows):\n",
        "                discovery_id = row.get('discoveryUrlId')\n",
        "\n",
        "                if discovery_id:\n",
        "                    print(f\"[{i+1}/{total}] Processing {discovery_id}...\")\n",
        "\n",
        "                    # Fetch Publications\n",
        "                    pub_json = get_publications(discovery_id)\n",
        "                    row['publications_json'] = pub_json\n",
        "\n",
        "                    # Small delay between calls\n",
        "                    time.sleep(0.7)\n",
        "\n",
        "                    # Fetch Grants\n",
        "                    grant_json = get_grants(discovery_id)\n",
        "                    row['grants_json'] = grant_json\n",
        "\n",
        "                    # Delay before next user\n",
        "                    time.sleep(0.7)\n",
        "                else:\n",
        "                    print(f\"[{i+1}/{total}] Skipping row with missing discoveryUrlId\")\n",
        "                    row['publications_json'] = '{}'\n",
        "                    row['grants_json'] = '{}'\n",
        "\n",
        "                writer.writerow(row)\n",
        "\n",
        "    print(f\"Done! Results saved to {output_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Input file can be the one with emails or the base one\n",
        "    # Assuming chained execution: base -> emails -> research\n",
        "    input_file = \"ufl_scholars_data_with_emails.csv\"\n",
        "\n",
        "    # Fallback to base if email file doesn't exist (e.g. if skipped)\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"'{input_file}' not found, falling back to 'ufl_scholars_data.csv'\")\n",
        "        input_file = \"ufl_scholars_data.csv\"\n",
        "\n",
        "    output_file = \"ufl_scholars_data_complete.csv\"\n",
        "\n",
        "    process_research_data(input_file, output_file)"
      ],
      "metadata": {
        "id": "-e9Xtv1A3oe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RdvqipX4eBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}